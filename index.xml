<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jane - A super concise theme for Hugo</title>
    <link>http://qsyqian.github.io/</link>
    <description>Recent content on Jane - A super concise theme for Hugo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 17 Sep 2019 15:08:18 +0800</lastBuildDate>
    
        <atom:link href="http://qsyqian.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Flannel 模式网络详解（vxlan）</title>
      <link>http://qsyqian.github.io/post/flannel-pod-communication-analysis/</link>
      <pubDate>Tue, 17 Sep 2019 15:08:18 +0800</pubDate>
      
      <guid>http://qsyqian.github.io/post/flannel-pod-communication-analysis/</guid>
      
        <description>

&lt;h2 id=&#34;flannel-模式网络详解-vxlan&#34;&gt;Flannel 模式网络详解（vxlan）&lt;/h2&gt;

&lt;p&gt;本文主要分析vxlan作为backend的flannel，如何实现跨宿主机通信。&lt;/p&gt;

&lt;p&gt;相关的flannel原理介绍的博客很多了，我就不分析原理，主要是介绍flannel安装完成之后，宿主机上的网络设备，以及跨宿主机的pod是如何实现一步步通信的。&lt;/p&gt;

&lt;h3 id=&#34;环境介绍&#34;&gt;环境介绍&lt;/h3&gt;

&lt;p&gt;本文使用的是三个节点的k8s，版本是1.13 。使用的flannel版本是0.10.0 。flannel配置的backend是vxlan。如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# kubectl get nodes
NAME              STATUS   ROLES         AGE     VERSION
sqian-k8s-node1   Ready    master,node   4h53m   v1.13.0
sqian-k8s-node2   Ready    master,node   4h52m   v1.13.0
sqian-k8s-node3   Ready    master,node   4h52m   v1.13.0
# kubectl exec -it kube-flannel-4rrms  -n kube-system -- /opt/bin/flanneld -version            
Defaulting container name to kube-flannel.
Use &#39;kubectl describe pod/kube-flannel-4rrms -n kube-system&#39; to see all of the containers in this pod.
v0.10.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;宿主机网络情况&#34;&gt;宿主机网络情况&lt;/h3&gt;

&lt;p&gt;与flannel相关的几个虚拟网络上设备：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;flannel.1：这是一个vxlan设备。也就是耳熟能详的vteh设备，负责网络数据包的封包和解封。&lt;/li&gt;
&lt;li&gt;cni0：是一个linux bridge，用于连接同一个宿主机上的pod。&lt;/li&gt;
&lt;li&gt;vethf12090da@if3：容器内eth0网卡的对端设备，从名字上看，在容器内eth0网卡的编号应为3。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面再看下上述网络设备的网络信息：&lt;/p&gt;

&lt;h4 id=&#34;flannel-1&#34;&gt;flannel.1：&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ifconfig flannel.1
flannel.1: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.168.0.0  netmask 255.255.255.255  broadcast 0.0.0.0
        inet6 fe80::e010:c0ff:fe12:aa5f  prefixlen 64  scopeid 0x20&amp;lt;link&amp;gt;
        ether e2:10:c0:12:aa:5f  txqueuelen 0  (Ethernet)
        RX packets 66  bytes 5544 (5.4 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 66  bytes 7524 (7.3 KiB)
        TX errors 0  dropped 8 overruns 0  carrier 0  collisions 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，flannel.1上有一个IP。（配置的容器的IP段位172.168.0.0/16），由于我们查看的node1上的flannel.1设备，而node1分配的subnet是172.168.0.0/24，该信息可以从node1的yaml文件中看到（这是因为默认的flannel使用的是kube subnet manager）。vxlan网络设备的原理这里不做赘述，感兴趣的可以看另外一篇&lt;a href=&#34;https://qsyqian.github.io/post/linux-vxlan-implement/&#34;&gt;博客&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;查看node1的yaml文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# kubectl get node sqian-k8s-node1 -o yaml | grep flannel
    flannel.alpha.coreos.com/backend-data: &#39;{&amp;quot;VtepMAC&amp;quot;:&amp;quot;e2:10:c0:12:aa:5f&amp;quot;}&#39;
    flannel.alpha.coreos.com/backend-type: vxlan
    flannel.alpha.coreos.com/kube-subnet-manager: &amp;quot;true&amp;quot;
    flannel.alpha.coreos.com/public-ip: 10.212.36.168
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见flannel在每个node的yaml文件中存储了下述信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;flannel.alpha.coreos.com/backend-data: vteh设备（flannel.1）的mac地址&lt;/li&gt;
&lt;li&gt;flannel.alpha.coreos.com/backend-type: backend type&lt;/li&gt;
&lt;li&gt;flannel.alpha.coreos.com/kube-subnet-manager: true 采用kube subnet manager&lt;/li&gt;
&lt;li&gt;flannel.alpha.coreos.com/public-ip: 互联IP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有了上述信息，flannel就可以在不同的node之间建立overlay网络，采用的就是vxlan技术。&lt;/p&gt;

&lt;p&gt;查看node1上的fdb表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# bridge fdb  |grep flannel.1
52:91:cb:aa:d1:bd dev flannel.1 dst 10.212.36.170 self permanent
02:74:40:05:9f:87 dev flannel.1 dst 10.212.36.169 self permanent
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述是node1上的fdb表，分别是转发到node2上和node3上。最后的permanent表示该fdb永远不会超时。从这里可以看出，该fdb是通过flannel来维护的，当集群中有新的node加入时，其上的flannel会申请一个新的subnet，该信息会通知到所有的flannel节点上，flannel会在fdb表中追加新的内容。&lt;/p&gt;

&lt;p&gt;删除一个节点的话，同理。也是会从该fdb表中修改对应的条目。&lt;/p&gt;

&lt;p&gt;查看node1上flannel.1设备的neigh：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip neigh show dev flannel.1
172.168.1.0 lladdr 02:74:40:05:9f:87 PERMANENT
172.168.2.0 lladdr 52:91:cb:aa:d1:bd PERMANENT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中172.168.1.0是node2上flannel.1的地址。&lt;/p&gt;

&lt;p&gt;172.168.2.0是node3上flannel.1的地址。&lt;/p&gt;

&lt;h4 id=&#34;cni0&#34;&gt;cni0&lt;/h4&gt;

&lt;p&gt;查看cni0的一些信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# brctl show cni0
bridge name     bridge id               STP enabled     interfaces
cni0            8000.0a58aca80001       no              veth0c81e625
                                                        veth3752bd50
                                                        vethf12090da
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见cni0是一个linux bridge设备，上面挂在了三个容器的网卡对端设备。&lt;/p&gt;

&lt;p&gt;查看其ip neigh：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip neigh show dev cni0
172.168.0.5 lladdr 0a:58:ac:a8:00:05 STALE
172.168.0.4 lladdr 0a:58:ac:a8:00:04 REACHABLE
172.168.0.6 lladdr 0a:58:ac:a8:00:06 STALE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述三个IP就是调度到该节点上的三个pod的IP。&lt;/p&gt;

&lt;h4 id=&#34;route信息&#34;&gt;route信息&lt;/h4&gt;

&lt;p&gt;最后来看一下宿主机上的路由信息&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip r
default via 10.212.36.254 dev eth0 ## 默认路由
10.212.36.0/24 dev eth0  proto kernel  scope link  src 10.212.36.168 ## 宿主机网卡的路由
169.254.0.0/16 dev eth0  scope link  metric 1002 
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 
172.168.0.0/24 dev cni0  proto kernel  scope link  src 172.168.0.1 ## （1）
172.168.1.0/24 via 172.168.1.0 dev flannel.1 onlink ## （2） 
172.168.2.0/24 via 172.168.2.0 dev flannel.1 onlink ## （3）
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;（1）：172.168.0.0/24是node1申请过来的subnet，理论上调度到该节点的pod都会从该C段地址内分配IP。因为所有的pod都是桥接到cni0上的，因此该条路由通过cni0转发给pod。&lt;/li&gt;
&lt;li&gt;（2）：172.168.1.0/24是node2申请过来的subnet，那么如果目的地址是该网段内的流量，则通过flannel.1设备发送给172.168.1.0，172.168.1.0是node2上的flannel.1的地址，flannel.1网卡将数据包vxlan封装之后发送给node2上的flannel.1。node1上的flannel.1是如何找到node2上的flannel.1呢，通过fdb表，上面已经说明。&lt;/li&gt;
&lt;li&gt;（3）同（2）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;容器内网络情况&#34;&gt;容器内网络情况&lt;/h3&gt;

&lt;p&gt;下面进入容器查看一下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# kubectl exec -it test-75b789cbdc-vdsg4 ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if12: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UP group default 
    link/ether 0a:58:ac:a8:00:06 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.168.0.6/24 scope global eth0
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;容器内的eth0网卡具有IP地址172.168.0.6，在宿主机申请的IP段（172.168.0.0/24）内。&lt;/p&gt;

&lt;p&gt;查看容器内的路由信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# kubectl exec -it test-75b789cbdc-vdsg4 ip r
default via 172.168.0.1 dev eth0 
172.168.0.0/24 dev eth0 proto kernel scope link src 172.168.0.6 
172.168.0.0/16 via 172.168.0.1 dev eth0 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认网关是172.168.0.1，这个地址是在cni0上的，将网桥作为自己的网关。&lt;/p&gt;

&lt;p&gt;如果都是172.168.0.0/24段内的通信，则直接走172.168.0.6。&lt;/p&gt;

&lt;p&gt;整个pod段（172.168.0.0/16）内的通信，走网关（cni0）地址172.168.0.1 。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;通过上述分析，集群内pod通信就很清晰了：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果是同一个节点上的pod通信，直接通过linux br转发即可；&lt;/li&gt;
&lt;li&gt;如果是跨节点pod通信，需要通过flannl.1 vxlan设备，封包之后发送给对端宿主机上的flannel.1设备。&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>Flannel源码分析</title>
      <link>http://qsyqian.github.io/post/flannel-source-code-analysis/</link>
      <pubDate>Thu, 12 Sep 2019 16:59:05 +0800</pubDate>
      
      <guid>http://qsyqian.github.io/post/flannel-source-code-analysis/</guid>
      
        <description>

&lt;h1 id=&#34;flannel源码分析&#34;&gt;Flannel源码分析&lt;/h1&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;flannel作为kubernetes的一种网络解决方案，在社区是比较活跃的。支持多种backend。&lt;/p&gt;

&lt;p&gt;flannel源码地址在：&lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;https://github.com/coreos/flannel&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;从官网上把flannel的代码clone下来，目录结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ll -ah
total 222K
drwxr-xr-x 1 user 197121    0 8月  30 18:37 ./
drwxr-xr-x 1 user 197121    0 8月  30 18:31 ../
-rw-r--r-- 1 user 197121  351 8月  30 18:33 .appveyor.yml
-rw-r--r-- 1 user 197121   56 8月  30 18:33 .dockerignore
drwxr-xr-x 1 user 197121    0 9月  17 09:34 .git/
drwxr-xr-x 1 user 197121    0 8月  30 18:33 .github/
-rw-r--r-- 1 user 197121  147 8月  30 18:33 .gitignore
drwxr-xr-x 1 user 197121    0 9月  17 11:22 .idea/
-rw-r--r-- 1 user 197121  411 8月  30 18:33 .travis.yml
drwxr-xr-x 1 user 197121    0 8月  30 18:33 backend/
-rw-r--r-- 1 user 197121 3.7K 8月  30 18:33 bill-of-materials.json
-rw-r--r-- 1 user 197121   98 8月  30 18:33 bill-of-materials.override.json
-rw-r--r-- 1 user 197121 3.1K 8月  30 18:33 code-of-conduct.md
-rw-r--r-- 1 user 197121 2.5K 8月  30 18:33 CONTRIBUTING.md
-rw-r--r-- 1 user 197121 1.5K 8月  30 18:33 DCO
drwxr-xr-x 1 user 197121    0 8月  30 18:33 dist/
-rw-r--r-- 1 user 197121  427 8月  30 18:33 Dockerfile.amd64
-rw-r--r-- 1 user 197121  504 8月  30 18:33 Dockerfile.arm
-rw-r--r-- 1 user 197121  494 8月  30 18:33 Dockerfile.arm64
-rw-r--r-- 1 user 197121  508 8月  30 18:33 Dockerfile.ppc64le
-rw-r--r-- 1 user 197121  504 8月  30 18:33 Dockerfile.s390x
drwxr-xr-x 1 user 197121    0 8月  30 18:33 Documentation/
-rw-r--r-- 1 user 197121  11K 8月  30 18:33 glide.lock
-rw-r--r-- 1 user 197121 1.6K 8月  30 18:33 glide.yaml
-rwxr-xr-x 1 user 197121  298 8月  30 18:33 header-check.sh*
drwxr-xr-x 1 user 197121    0 8月  30 18:33 images/
-rw-r--r-- 1 user 197121  12K 8月  30 18:33 LICENSE
drwxr-xr-x 1 user 197121    0 8月  30 18:33 logos/
-rw-r--r-- 1 user 197121  21K 8月  30 18:33 main.go
-rw-r--r-- 1 user 197121   98 8月  30 18:33 MAINTAINERS
-rw-r--r-- 1 user 197121  12K 8月  30 18:33 Makefile
drwxr-xr-x 1 user 197121    0 8月  30 18:33 network/
-rw-r--r-- 1 user 197121  131 8月  30 18:33 NOTICE
-rw-r--r-- 1 user 197121  212 8月  30 18:33 OWNERS
-rw-r--r-- 1 user 197121  75K 8月  30 18:33 packet-01.png
drwxr-xr-x 1 user 197121    0 8月  30 18:33 pkg/
-rw-r--r-- 1 user 197121 4.3K 8月  30 18:33 README.md
drwxr-xr-x 1 user 197121    0 8月  30 18:33 subnet/
drwxr-xr-x 1 user 197121    0 8月  30 18:33 vendor/
drwxr-xr-x 1 user 197121    0 8月  30 18:33 version/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里介绍几个主要目录的用途：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;backend，flannel支持多种flannel，比如host-gw、vxlan等&lt;/li&gt;
&lt;li&gt;dist，一些脚本和dockerfile文件&lt;/li&gt;
&lt;li&gt;network，对主机的iptables进行操作&lt;/li&gt;
&lt;li&gt;pkg，外部可引用的包&lt;/li&gt;
&lt;li&gt;subnet，flannel中每个host对应一个subnet&lt;/li&gt;
&lt;li&gt;./main.go，flannel程序的入口&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;经过上述分析，分析flannel的源代码就从./main.go文件入手。&lt;/p&gt;

&lt;h2 id=&#34;1-参数解析&#34;&gt;1.参数解析&lt;/h2&gt;

&lt;p&gt;./main.go中结构体CmdLineOpts包含了所有命令行传入的参数解析，具体如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type CmdLineOpts struct {
	etcdEndpoints          string
	etcdPrefix             string
	etcdKeyfile            string
	etcdCertfile           string
	etcdCAFile             string
	etcdUsername           string
	etcdPassword           string
    // 上述的etcd参数，指的是如果采用etcd作为后端subnet存储的话，要提供上述etcd连接参数
	help                   bool
	version                bool
    // 帮助和打印出version
	kubeSubnetMgr          bool
    // 如果上述kubeSubnetMgr为true，则使用kube-apiserver为subnet的后端存储（其实是将每个node的subnet信息存入到了node的annotations中）
	kubeApiUrl             string
	kubeAnnotationPrefix   string
	kubeConfigFile         string
    // 上述三个kube相关的参数标识连接kube-apiserver的必要参数
	iface                  flagSlice
	ifaceRegex             flagSlice
    // iface参数，用来作为多节点通信的网卡，不指定的话，flannel会使用默认网卡和其上IP信息
	ipMasq                 bool
    // ipMasq （具体意义待定）
	subnetFile             string
    // 存储subnet的文件，默认为/run/flannel/subnet.env，其中存储了整个集群的pod subnet段，本机上pod的subnet段等信息
	subnetDir              string
	publicIP               string
    // 用来跨节点通信的IP地址，和上述iface参数类似
	subnetLeaseRenewMargin int
    // subnet是以lease（租约）的形式存储起来的，到期会自动过期。需要定时续租，改时间表示在lease过期前多少时间续租，单位是min，默认值是60
	healthzIP              string
	healthzPort            int
    // 用来健康检查的IP和PORT
	charonExecutablePath   string
	charonViciUri          string
	iptablesResyncSeconds  int
    // 同步iptables规则的时间间隔。默认5s同步一次
	iptablesForwardRules   bool
    // add default accept rules to FORWARD chain in iptables
	netConfPath            string
    // networkConfiguration PATH
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;main.go中首先是init函数对命令行传入参数进行赋值，没有赋值的采用默认值。然后进到main()函数中。&lt;/p&gt;

&lt;h2 id=&#34;2-main函数结构&#34;&gt;2.main函数结构&lt;/h2&gt;

&lt;p&gt;首先看一下main函数的主要结构，然后分步分析：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {
	// 如果是打印版本，则打印退出
	if opts.version {
		fmt.Fprintln(os.Stderr, version.Version)
		os.Exit(0)
	}

	flagutil.SetFlagsFromEnv(flannelFlags, &amp;quot;FLANNELD&amp;quot;)

	// Validate flags
    // 验证参数，如果续租时间大于24*60或者小于等于0，则直接报错。
    // 因为默认的租约时间为24*60
	if opts.subnetLeaseRenewMargin &amp;gt;= 24*60 || opts.subnetLeaseRenewMargin &amp;lt;= 0 {
		log.Error(&amp;quot;Invalid subnet-lease-renew-margin option, out of acceptable range&amp;quot;)
		os.Exit(1)
	}
    
    // Work out which interface to use
    // 找出externalInterface，很简单，如果命令行参数指定了使用哪个网卡，则直接使用，若没指定，则需要自己去找默认的网卡，主要是LookupExtIface这个函数来寻找。这个函数中所调用的ip.GetDefaultGatewayIface()是位于pkg目录下，在自己写程序来获取linux主机默认网卡和ip信息的时候也可以直接引用该函数
	var extIface *backend.ExternalInterface
	var err error
	// Check the default interface only if no interfaces are specified
	if len(opts.iface) == 0 &amp;amp;&amp;amp; len(opts.ifaceRegex) == 0 {
		extIface, err = LookupExtIface(&amp;quot;&amp;quot;, &amp;quot;&amp;quot;)
		if err != nil {
			log.Error(&amp;quot;Failed to find any valid interface to use: &amp;quot;, err)
			os.Exit(1)
		}
	} else {
		// Check explicitly specified interfaces
		for _, iface := range opts.iface {
			extIface, err = LookupExtIface(iface, &amp;quot;&amp;quot;)
			if err != nil {
				log.Infof(&amp;quot;Could not find valid interface matching %s: %s&amp;quot;, iface, err)
			}

			if extIface != nil {
				break
			}
		}

		// Check interfaces that match any specified regexes
		if extIface == nil {
			for _, ifaceRegex := range opts.ifaceRegex {
				extIface, err = LookupExtIface(&amp;quot;&amp;quot;, ifaceRegex)
				if err != nil {
					log.Infof(&amp;quot;Could not find valid interface matching %s: %s&amp;quot;, ifaceRegex, err)
				}

				if extIface != nil {
					break
				}
			}
		}

		if extIface == nil {
			// Exit if any of the specified interfaces do not match
			log.Error(&amp;quot;Failed to find interface to use that matches the interfaces and/or regexes provided&amp;quot;)
			os.Exit(1)
		}
	}
    
    // 第一步：创建SubnetManager，主要是用来管理subnet的。
    sm, err := newSubnetManager()
	if err != nil {
		log.Error(&amp;quot;Failed to create SubnetManager: &amp;quot;, err)
		os.Exit(1)
	}
	log.Infof(&amp;quot;Created subnet manager: %s&amp;quot;, sm.Name())
    
    // 下述这段代码是控制程序优雅退出的，首先是创建了sigs channel，类型为os.Signal,buffer为1.
    // 此处两个golang的用法，第一个是context，第二个是sync.waitGroup
    // Register for SIGINT and SIGTERM
	log.Info(&amp;quot;Installing signal handlers&amp;quot;)
	sigs := make(chan os.Signal, 1)
	signal.Notify(sigs, os.Interrupt, syscall.SIGTERM)

	// This is the main context that everything should run in.
	// All spawned goroutines should exit when cancel is called on this context.
	// Go routines spawned from main.go coordinate using a WaitGroup. This provides a mechanism to allow the shutdownHandler goroutine
	// to block until all the goroutines return . If those goroutines spawn other goroutines then they are responsible for
	// blocking and returning only when cancel() is called.
	// 创建全局的ctx
    ctx, cancel := context.WithCancel(context.Background())
	
    // 创建任务
    wg := sync.WaitGroup{}

   // 添加一个任务
	wg.Add(1)
	go func() {
        // shutdownHander中如果监听到系统信号，则调用cancel函数，那么所有的调用都将终止
		shutdownHandler(ctx, sigs, cancel)
		wg.Done()
	}()
    
    // 如果定义了健康检查端口，则要启动一个http服务监听该端口
    if opts.healthzPort &amp;gt; 0 {
		// It&#39;s not super easy to shutdown the HTTP server so don&#39;t attempt to stop it cleanly
		go mustRunHealthz()
	}
    
    // 第二步：创建backend，并且通过该backend来注册subnet
    // Create a backend manager then use it to create the backend and register the network with it.
	bm := backend.NewManager(ctx, sm, extIface)
	be, err := bm.GetBackend(config.BackendType)
	if err != nil {
		log.Errorf(&amp;quot;Error fetching backend: %s&amp;quot;, err)
		cancel()
		wg.Wait()
		os.Exit(1)
	}

    // 注册subnet
	bn, err := be.RegisterNetwork(ctx, wg, config)
	if err != nil {
		log.Errorf(&amp;quot;Error registering network: %s&amp;quot;, err)
		cancel()
		wg.Wait()
		os.Exit(1)
	}
    
    // 第三步：如果指定了ipMasq，则定期同步iptables
    // Set up ipMasq if needed
	if opts.ipMasq {
		if err = recycleIPTables(config.Network, bn.Lease()); err != nil {
			log.Errorf(&amp;quot;Failed to recycle IPTables rules, %v&amp;quot;, err)
			cancel()
			wg.Wait()
			os.Exit(1)
		}
		log.Infof(&amp;quot;Setting up masking rules&amp;quot;)
		go network.SetupAndEnsureIPTables(network.MasqRules(config.Network, bn.Lease()), opts.iptablesResyncSeconds)
	}
    
    // 第四步：iptablesForwardRules指定的话，定期去同步指定的iptables
    // Always enables forwarding rules. This is needed for Docker versions &amp;gt;1.13 (https://docs.docker.com/engine/userguide/networking/default_network/container-communication/#container-communication-between-hosts)
	// In Docker 1.12 and earlier, the default FORWARD chain policy was ACCEPT.
	// In Docker 1.13 and later, Docker sets the default policy of the FORWARD chain to DROP.
	if opts.iptablesForwardRules {
		log.Infof(&amp;quot;Changing default FORWARD chain policy to ACCEPT&amp;quot;)
		go network.SetupAndEnsureIPTables(network.ForwardRules(config.Network.String()), opts.iptablesResyncSeconds)
	}
    
    // 第五步：写入subnet file
    if err := WriteSubnetFile(opts.subnetFile, config.Network, opts.ipMasq, bn); err != nil {
		// Continue, even though it failed.
		log.Warningf(&amp;quot;Failed to write subnet file: %s&amp;quot;, err)
	} else {
		log.Infof(&amp;quot;Wrote subnet file to %s&amp;quot;, opts.subnetFile)
	}
    
    // 第六步：backend run起来
    // Start &amp;quot;Running&amp;quot; the backend network. This will block until the context is done so run in another goroutine.
	log.Info(&amp;quot;Running backend.&amp;quot;)
	wg.Add(1)
	go func() {
		bn.Run(ctx)
		wg.Done()
	}()
    
    // 第七步：kube subnet mgr监听subnet lease
    // Kube subnet mgr doesn&#39;t lease the subnet for this node - it just uses the podCidr that&#39;s already assigned.
	if !opts.kubeSubnetMgr {
		err = MonitorLease(ctx, sm, bn, &amp;amp;wg)
		if err == errInterrupted {
			// The lease was &amp;quot;revoked&amp;quot; - shut everything down
			cancel()
		}
	}

    
    log.Info(&amp;quot;Waiting for all goroutines to exit&amp;quot;)
	// Block waiting for all the goroutines to finish.
	wg.Wait()
	log.Info(&amp;quot;Exiting cleanly...&amp;quot;)
	os.Exit(0)
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述main函数中列出了7个主要的步骤，下面依次分析。&lt;/p&gt;

&lt;h2 id=&#34;3-第一步-创建subnetmanager&#34;&gt;3.第一步：创建SubnetManager&lt;/h2&gt;

&lt;p&gt;main.go中的newSubnetManager函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func newSubnetManager() (subnet.Manager, error) {
	if opts.kubeSubnetMgr {
		return kube.NewSubnetManager(opts.kubeApiUrl, opts.kubeConfigFile, opts.kubeAnnotationPrefix, opts.netConfPath)
	}

	cfg := &amp;amp;etcdv2.EtcdConfig{
		Endpoints: strings.Split(opts.etcdEndpoints, &amp;quot;,&amp;quot;),
		Keyfile:   opts.etcdKeyfile,
		Certfile:  opts.etcdCertfile,
		CAFile:    opts.etcdCAFile,
		Prefix:    opts.etcdPrefix,
		Username:  opts.etcdUsername,
		Password:  opts.etcdPassword,
	}

	// Attempt to renew the lease for the subnet specified in the subnetFile
	prevSubnet := ReadCIDRFromSubnetFile(opts.subnetFile, &amp;quot;FLANNEL_SUBNET&amp;quot;)

	return etcdv2.NewLocalManager(cfg, prevSubnet)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果使用了kubeSubnetMgr，则调用kube.NewSubnetManager，否则调用etcdv2。这一点需要说明的是etcd发展到现在支持不同的版本，v2和v3，且v3版本具有很多优势。flannel仓库中v2版本相关的代码提交是2 years之前，因此可见目前flannel实际上只支持kube-apiserver作为后端存储，也是默认的存储方式。&lt;/p&gt;

&lt;p&gt;./flannel/subnet/kube/kube.go：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type kubeSubnetManager struct {
	// 往node的yaml文件中添加的annotations
	annotations    annotations
	// 与kube-apiserver交互的client
	client         clientset.Interface
    // 运行在哪个node上
	nodeName       string
    // node lister
	nodeStore      listers.NodeLister
    // node controller
	nodeController cache.Controller
	subnetConf     *subnet.Config
	events         chan subnet.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面进入flannel/subnet/kube/kube.go里的NewSubnetManager函数中，看看都干了啥：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func NewSubnetManager(apiUrl, kubeconfig, prefix, netConfPath string) (subnet.Manager, error) {
	//先看看入参函数：kube-apiserver的地址，kubeconfig的地址，prefix是给node打anno的时候前缀，netConfPath是配置存储目录，实际上安装完成之后，是通过configmap的形式挂载到/etc/kube-flannel/net-conf.json里的
	var cfg *rest.Config
	var err error
	// Try to build kubernetes config from a master url or a kubeconfig filepath. If neither masterUrl
	// or kubeconfigPath are passed in we fall back to inClusterConfig. If inClusterConfig fails,
	// we fallback to the default config.
	cfg, err = clientcmd.BuildConfigFromFlags(apiUrl, kubeconfig)
	if err != nil {
		return nil, fmt.Errorf(&amp;quot;fail to create kubernetes config: %v&amp;quot;, err)
	}

	c, err := clientset.NewForConfig(cfg)
	if err != nil {
		return nil, fmt.Errorf(&amp;quot;unable to initialize client: %v&amp;quot;, err)
	}
    // 上述是通过提供的配置，创建clientset，用来连接kube-apiserver

	// The kube subnet mgr needs to know the k8s node name that it&#39;s running on so it can annotate it.
	// If we&#39;re running as a pod then the POD_NAME and POD_NAMESPACE will be populated and can be used to find the node
	// name. Otherwise, the environment variable NODE_NAME can be passed in.
	// 获取nodename
    nodeName := os.Getenv(&amp;quot;NODE_NAME&amp;quot;)
	if nodeName == &amp;quot;&amp;quot; {
		podName := os.Getenv(&amp;quot;POD_NAME&amp;quot;)
		podNamespace := os.Getenv(&amp;quot;POD_NAMESPACE&amp;quot;)
		if podName == &amp;quot;&amp;quot; || podNamespace == &amp;quot;&amp;quot; {
			return nil, fmt.Errorf(&amp;quot;env variables POD_NAME and POD_NAMESPACE must be set&amp;quot;)
		}

		pod, err := c.Pods(podNamespace).Get(podName, metav1.GetOptions{})
		if err != nil {
			return nil, fmt.Errorf(&amp;quot;error retrieving pod spec for &#39;%s/%s&#39;: %v&amp;quot;, podNamespace, podName, err)
		}
		nodeName = pod.Spec.NodeName
		if nodeName == &amp;quot;&amp;quot; {
			return nil, fmt.Errorf(&amp;quot;node name not present in pod spec &#39;%s/%s&#39;&amp;quot;, podNamespace, podName)
		}
	}

    // 读取网络配置文件
	netConf, err := ioutil.ReadFile(netConfPath)
	if err != nil {
		return nil, fmt.Errorf(&amp;quot;failed to read net conf: %v&amp;quot;, err)
	}

    // 解析网络配置
	sc, err := subnet.ParseConfig(string(netConf))
	if err != nil {
		return nil, fmt.Errorf(&amp;quot;error parsing subnet config: %s&amp;quot;, err)
	}

    // 初始化并运行subnetManager，传入的参数有clientset，网络配置，nodename，anno前缀，具体的new和Run函数下文分析
	sm, err := newKubeSubnetManager(c, sc, nodeName, prefix)
	if err != nil {
		return nil, fmt.Errorf(&amp;quot;error creating network manager: %s&amp;quot;, err)
	}
	go sm.Run(context.Background())

    // 下面确保hasSynced
	glog.Infof(&amp;quot;Waiting %s for node controller to sync&amp;quot;, nodeControllerSyncTimeout)
	err = wait.Poll(time.Second, nodeControllerSyncTimeout, func() (bool, error) {
		return sm.nodeController.HasSynced(), nil
	})
	if err != nil {
		return nil, fmt.Errorf(&amp;quot;error waiting for nodeController to sync state: %v&amp;quot;, err)
	}
	glog.Infof(&amp;quot;Node controller sync successful&amp;quot;)

	return sm, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面去看看如何newKubeSubnetManager和让其Run起来的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func newKubeSubnetManager(c clientset.Interface, sc *subnet.Config, nodeName, prefix string) (*kubeSubnetManager, error) {
   var err error
   var ksm kubeSubnetManager
   ksm.annotations, err = newAnnotations(prefix)
   if err != nil {
      return nil, err
   }
   ksm.client = c
   ksm.nodeName = nodeName
   ksm.subnetConf = sc
   ksm.events = make(chan subnet.Event, 5000)
   indexer, controller := cache.NewIndexerInformer(
      &amp;amp;cache.ListWatch{
         ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
            return ksm.client.CoreV1().Nodes().List(options)
         },
         WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
            return ksm.client.CoreV1().Nodes().Watch(options)
         },
      },
      &amp;amp;v1.Node{},
      resyncPeriod,
      cache.ResourceEventHandlerFuncs{
         AddFunc: func(obj interface{}) {
            ksm.handleAddLeaseEvent(subnet.EventAdded, obj)
         },
         UpdateFunc: ksm.handleUpdateLeaseEvent,
         DeleteFunc: func(obj interface{}) {
            node, isNode := obj.(*v1.Node)
            // We can get DeletedFinalStateUnknown instead of *api.Node here and we need to handle that correctly.
            if !isNode {
               deletedState, ok := obj.(cache.DeletedFinalStateUnknown)
               if !ok {
                  glog.Infof(&amp;quot;Error received unexpected object: %v&amp;quot;, obj)
                  return
               }
               node, ok = deletedState.Obj.(*v1.Node)
               if !ok {
                  glog.Infof(&amp;quot;Error deletedFinalStateUnknown contained non-Node object: %v&amp;quot;, deletedState.Obj)
                  return
               }
               obj = node
            }
            ksm.handleAddLeaseEvent(subnet.EventRemoved, obj)
         },
      },
      cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc},
   )
   ksm.nodeController = controller
   ksm.nodeStore = listers.NewNodeLister(indexer)
   return &amp;amp;ksm, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总结一下上述函数都干了啥：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初始化kubeSubnetManager&lt;/li&gt;
&lt;li&gt;ksm主要是对集群中node进行监听，因为flannel是根据node来划分网段的&lt;/li&gt;
&lt;li&gt;根据监听到的node的事件，放入到ksm的events channel中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此外，ksm.Run函数就是运行起来。&lt;/p&gt;

&lt;h2 id=&#34;4-第二步-创建backend&#34;&gt;4.第二步：创建backend&lt;/h2&gt;

&lt;p&gt;首先看backend/manager.go中的NewManager函数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func NewManager(ctx context.Context, sm subnet.Manager, extIface *ExternalInterface) Manager {
   return &amp;amp;manager{
      ctx:      ctx,
      sm:       sm,
      extIface: extIface,
      active:   make(map[string]Backend),
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只是返回了一个manager对象：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type manager struct {
   ctx      context.Context
    // 上文创建好的ksm
   sm       subnet.Manager
    // 用来与外界通信的interface
   extIface *ExternalInterface
    // 互斥锁
   mux      sync.Mutex
   active   map[string]Backend
   wg       sync.WaitGroup
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面看bm.GetBackend(config.BackendType)函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (bm *manager) GetBackend(backendType string) (Backend, error) {
    //传入的参数是config的BackendType，此配置项是通过configmap写入到flannel的pod中的，因此是一个固定的值，比如vxlan
   bm.mux.Lock()
   defer bm.mux.Unlock()

   betype := strings.ToLower(backendType)
   // see if one is already running
   if be, ok := bm.active[betype]; ok {
      return be, nil
   }

   // first request, need to create and run it
   befunc, ok := constructors[betype]
   if !ok {
      return nil, fmt.Errorf(&amp;quot;unknown backend type: %v&amp;quot;, betype)
   }

   be, err := befunc(bm.sm, bm.extIface)
   if err != nil {
      return nil, err
   }
   bm.active[betype] = be

   bm.wg.Add(1)
   go func() {
      &amp;lt;-bm.ctx.Done()

      // TODO(eyakubovich): this obviosly introduces a race.
      // GetBackend() could get called while we are here.
      // Currently though, all backends&#39; Run exit only
      // on shutdown

      bm.mux.Lock()
      delete(bm.active, betype)
      bm.mux.Unlock()

      bm.wg.Done()
   }()

   return be, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;main.go的289行bn, err := be.RegisterNetwork(ctx, wg, config)，实际上调用的是具体的backend的Register，比如配置了vxlan就调用vxlan的register：&lt;/p&gt;

&lt;p&gt;flannel/backend/vxlan/vxlan.go:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (be *VXLANBackend) RegisterNetwork(ctx context.Context, wg sync.WaitGroup, config *subnet.Config) (backend.Network, error) {
   // Parse our configuration
   cfg := struct {
      VNI           int
      Port          int
      GBP           bool
      Learning      bool
      DirectRouting bool
   }{
      VNI: defaultVNI,
   }

   if len(config.Backend) &amp;gt; 0 {
      if err := json.Unmarshal(config.Backend, &amp;amp;cfg); err != nil {
         return nil, fmt.Errorf(&amp;quot;error decoding VXLAN backend config: %v&amp;quot;, err)
      }
   }
   log.Infof(&amp;quot;VXLAN config: VNI=%d Port=%d GBP=%v Learning=%v DirectRouting=%v&amp;quot;, cfg.VNI, cfg.Port, cfg.GBP, cfg.Learning, cfg.DirectRouting)

   devAttrs := vxlanDeviceAttrs{
      vni:       uint32(cfg.VNI),
      name:      fmt.Sprintf(&amp;quot;flannel.%v&amp;quot;, cfg.VNI),
      vtepIndex: be.extIface.Iface.Index,
      vtepAddr:  be.extIface.IfaceAddr,
      vtepPort:  cfg.Port,
      gbp:       cfg.GBP,
      learning:  cfg.Learning,
   }

   dev, err := newVXLANDevice(&amp;amp;devAttrs)
   if err != nil {
      return nil, err
   }
   dev.directRouting = cfg.DirectRouting

   subnetAttrs, err := newSubnetAttrs(be.extIface.ExtAddr, dev.MACAddr())
   if err != nil {
      return nil, err
   }

   lease, err := be.subnetMgr.AcquireLease(ctx, subnetAttrs)
   switch err {
   case nil:
   case context.Canceled, context.DeadlineExceeded:
      return nil, err
   default:
      return nil, fmt.Errorf(&amp;quot;failed to acquire lease: %v&amp;quot;, err)
   }

   // Ensure that the device has a /32 address so that no broadcast routes are created.
   // This IP is just used as a source address for host to workload traffic (so
   // the return path for the traffic has an address on the flannel network to use as the destination)
   if err := dev.Configure(ip.IP4Net{IP: lease.Subnet.IP, PrefixLen: 32}); err != nil {
      return nil, fmt.Errorf(&amp;quot;failed to configure interface %s: %s&amp;quot;, dev.link.Attrs().Name, err)
   }

   return newNetwork(be.subnetMgr, be.extIface, dev, ip.IP4Net{}, lease)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数主要是创建vxlan设备。配置对应的路由，给vxlan设备配置IP信息等等。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>手动创建vxlan网络连接container</title>
      <link>http://qsyqian.github.io/post/linux-vxlan-implement/</link>
      <pubDate>Tue, 27 Aug 2019 10:57:51 +0800</pubDate>
      
      <guid>http://qsyqian.github.io/post/linux-vxlan-implement/</guid>
      
        <description>

&lt;h1 id=&#34;手动创建vxlan网络连接container&#34;&gt;手动创建vxlan网络连接container&lt;/h1&gt;

&lt;p&gt;​   本文主要对linux下实现vxlan网络有个初步的认识，涉及较多的是动手操作，而不是理论知识。如果想知道更多关于vxlan协议的一些理论，包括产生的北京，报文的格式，请参考&lt;a href=&#34;https://cizixs.com/2017/09/25/vxlan-protocol-introduction&#34;&gt;vxlan 协议的介绍文章&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;​   在下述内容中我们主要是在linux上模拟vxlan设备，实现docker跨主机的overlay网络。&lt;/p&gt;

&lt;h2 id=&#34;实验环境&#34;&gt;实验环境&lt;/h2&gt;

&lt;p&gt;本次实验我们使用两个linux机器，系统为centos7，IP信息如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;192.168.1.100&lt;/li&gt;
&lt;li&gt;192.168.1.200&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​   我们搭建的overlay网络的网段为172.55.1.0/24 。实验目的就是vxlan能够通过overlay IP互相连接。&lt;/p&gt;

&lt;h2 id=&#34;实验&#34;&gt;实验&lt;/h2&gt;

&lt;h3 id=&#34;点对点的vxlan&#34;&gt;点对点的vxlan&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;先来搭建一个最简单的 vxlan 网络，两台机器构成一个 vxlan 网络，每台机器上有一个 vtep，vtep 通过它们的 IP 互相通信。这次实验完成后的网络结构如下图所示：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://tva1.sinaimg.cn/large/007X8olVly1g6wqzd5x70j30fl07vweg.jpg&#34; alt=&#34;1566869818907&#34; /&gt;&lt;/p&gt;

&lt;p&gt;首先使用 &lt;code&gt;ip&lt;/code&gt; 命令创建我们的 vxlan interface，在192.168.1.100上执行下述命令：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ip link add vxlan1 type vxlan id 444 dstport 4789 remote 192.168.1.200 local 192.168.1.100 dev eth0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;大概解释下上述参数的含义：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;vxlan1：所创建的网络设备的名字&lt;/li&gt;
&lt;li&gt;type vxlan：所创建的网络设备的类型&lt;/li&gt;
&lt;li&gt;id 444：所创建vxlan VNI，这个值可以在 1 到 2^24 之间&lt;/li&gt;
&lt;li&gt;dstport 4789:vtep 通信的端口，linux 默认使用 8472（为了保持兼容，默认值一直没有更改），而 IANA 分配的端口是 4789，所以我们这里显式指定了它的值&lt;/li&gt;
&lt;li&gt;remote 192.168.1.200: 对方 vtep 的地址，类似于点对点协议&lt;/li&gt;
&lt;li&gt;local 192.168.1.100: 当前节点 vtep 要使用的 IP 地址&lt;/li&gt;
&lt;li&gt;dev eth0: 当节点用于 vtep 通信的网卡设备，用来读取 IP 地址。注意这个参数和 &lt;code&gt;local&lt;/code&gt; 参数含义是相同的，在这里写出来是为了告诉大家有两个参数存在&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;执行完之后会在宿主机上生成对应的网络设备，可以通过ip命令查看该网络设备的详情：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip -d link show dev vxlan1
1044: vxlan1: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1400 qdisc noop state DOWN mode DEFAULT 
    link/ether ba:08:0a:dd:3f:71 brd ff:ff:ff:ff:ff:ff promiscuity 0 
    vxlan id 444 remote 192.168.1.200 local 192.168.1.100 dev eth0 srcport 0 0 dstport 4789 ageing 300 addrgenmode eui64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给vxlan1配置ip并启用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip addr add 172.55.1.2/24 dev vxlan1
# ip link set vxlan1 up
# ifconfig vxlan1
vxlan1: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1400
        inet 172.55.1.2  netmask 255.255.255.0  broadcast 0.0.0.0
        inet6 fe80::b808:aff:fedd:3f71  prefixlen 64  scopeid 0x20&amp;lt;link&amp;gt;
        ether ba:08:0a:dd:3f:71  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 9  bytes 806 (806.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次观察宿主机上几个点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;路由变化&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip r |grep vxlan1
172.55.1.0/24 dev vxlan1  proto kernel  scope link  src 172.55.1.2 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vxlan fdb表变化&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# bridge fdb| grep vxlan1
00:00:00:00:00:00 dev vxlan1 dst 192.168.1.200 via eth0 self permanent
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个表项的意思是说，默认的而 vtep 对端地址为 &lt;code&gt;192.168.1.200&lt;/code&gt;，换句话说，如果接收到的报文添加上 vxlan 头部之后都会发到 &lt;code&gt;192.168.1.200&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在另外一台虚拟机（&lt;code&gt;192.168.1.200&lt;/code&gt;）上也进行相同的配置，要保证 VNI 也是 444，dstport 也是 4789，并修改 vtep 的地址和 remote IP 地址到相应的值。测试两台 vtep 的连通性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 在192.168.1.100上操作
# ping -c 3 172.55.1.3
PING 172.55.1.3 (172.55.1.3) 56(84) bytes of data.
64 bytes from 172.55.1.3: icmp_seq=1 ttl=64 time=2.14 ms
64 bytes from 172.55.1.3: icmp_seq=2 ttl=64 time=0.511 ms
64 bytes from 172.55.1.3: icmp_seq=3 ttl=64 time=0.599 ms

--- 172.55.1.3 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2001ms
rtt min/avg/max/mdev = 0.511/1.083/2.140/0.748 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么问题来了，这两个vxlan设备是可以正常通信，怎么证明其走的就是隧道网络呢？其实可以通过抓包来看包的头部包含哪些内容来判断。&lt;/p&gt;

&lt;h3 id=&#34;多播模式的vxlan&#34;&gt;多播模式的vxlan&lt;/h3&gt;

&lt;p&gt;​   上述实现了点对点的vxlan，其实没啥大用。因为一般集群不止两个节点，因此多播模式的vxlan需要研究一下。&lt;/p&gt;

&lt;p&gt;​   这个实验和前面一个非常相似，只不过主机之间不是点对点的连接，而是通过多播组成一个虚拟的整体。最终的网络架构也很相似（为了简单图中只有两个主机，但这个模型可以容纳多个主机组成 vxlan 网络）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://tva1.sinaimg.cn/large/007X8olVly1g6wr0twgasj30fl0aw74b.jpg&#34; alt=&#34;1566870232376&#34; /&gt;&lt;/p&gt;

&lt;p&gt;​   同样的，先创建vxlan设备，命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip link add vxlan1 type vxlan id 444 dstport 4789 group 239.1.1.1  dev eth0
# ip addr add 172.55.1.2/24 dev vxlan1
# ip link set vxlan1 up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   这个命令和上述点对点的vxlan设备创建命令相似，主要不同的是group，很容易理解，点对点的时候，只需要指明对端的地址就行。而在多播模式下，需要把不同的主机加到一个组（group）内，因此需要制定一个group address。关于多播的原理和使用不是这篇文章的重点，这里选择的多播 IP 地址也没有特殊的含义，关于多播的内容可以自行了解。&lt;/p&gt;

&lt;p&gt;​   上述命令运行完之后，在宿主机上路由不变，和点对点的一样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip r | grep vxlan1                  
172.55.1.0/24 dev vxlan1  proto kernel  scope link  src 172.55.1.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   不同的是bridge fdb信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# bridge fdb |grep vxlan1 
00:00:00:00:00:00 dev vxlan1 dst 239.1.1.1 via eth0 self permanent
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   这里默认表项的 dst 字段的值变成了多播地址 &lt;code&gt;239.1.1.1&lt;/code&gt;，而不是之前对方的 vtep 地址。同理给所有需要通信的节点进行上述配置，可以验证他们能通过 172.55..1.0/24 网络互相访问。&lt;/p&gt;

&lt;p&gt;​   我们来分析这个模式下 vxlan 通信的过程：&lt;/p&gt;

&lt;p&gt;​   在配置完成之后，所有linux机器的vtep 通过 IGMP 加入同一个多播网络 &lt;code&gt;239.1.1.1&lt;/code&gt;。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;发送 ping 报文到 &lt;code&gt;172.55.1.3&lt;/code&gt;，查看路由表，报文会从 vxlan1 发出去&lt;/li&gt;
&lt;li&gt;内核发现 vxlan1 的 IP 是 &lt;code&gt;172.55.1.2/24&lt;/code&gt;，和目的 IP 在同一个网段，所以在同一个局域网，需要知道对方的 MAC 地址，因此会发送 ARP 报文查询&lt;/li&gt;
&lt;li&gt;ARP 报文源 MAC 地址为 vxlan1 的 MAC 地址，目的 MAC 地址为全 1 的广播地址&lt;/li&gt;
&lt;li&gt;vxlan 根据配置（VNI 444）添加上头部&lt;/li&gt;
&lt;li&gt;因为不知道对方 vtep 在哪台主机上，根据配置，vtep 会往多播地址 239.1.1.1 发送多播报文&lt;/li&gt;
&lt;li&gt;多播组中所有的主机都会受到这个报文，内核发现是 vxlan 报文，会根据 VNI 发送给对应的 vtep&lt;/li&gt;
&lt;li&gt;vtep 去掉 vxlan 头部，取出真正的 ARP 请求报文。同时 vtep 会记录 &lt;code&gt;&amp;lt;源 MAC 地址 - vtep 所在主机 IP 地址&amp;gt;&lt;/code&gt; 信息到 fdb 表中&lt;/li&gt;
&lt;li&gt;如果发现 ARP 不是发送给自己的，直接丢弃；如果是发送给自己的，则生成 ARP 应答报文&lt;/li&gt;
&lt;li&gt;应答报文目的 MAC 地址是发送方 vtep 的 MAC 地址，而且 vtep 已经通过源报文学习到了 vtep 所在的主机，因此会直接单播发送给目的 vtep。因此 vtep 不需要多播，就能填充所有的头部信息&lt;/li&gt;
&lt;li&gt;应答报文通过 underlay 网络直接返回给发送方主机，发送方主机根据 VNI 把报文转发给 vtep，vtep 解包取出 ARP 应答报文，添加 arp 缓存到内核。并根据报文学习到目的 vtep 所在的主机地址，添加到 fdb 表中&lt;/li&gt;
&lt;li&gt;vtep 已经知道了通信需要的所有信息，后续 ICMP 的 ping 报文都是单播进行的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;​   在这个过程中，在主机上抓包更容易看到通信的具体情况，下面是 ARP 请求报文的详情：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws3.sinaimg.cn/large/006tKfTcgy1fjyb6b5ybdj30x20lq11q.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到 vxlan 报文可以分为三块：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;里层（图中最下面）是 overlay 网络中实体看到的报文（比如这里的 ARP 请求），它们和经典网络的通信报文没有任何区别，除了因为 MTU 导致有些报文比较小&lt;/li&gt;
&lt;li&gt;然后是 vxlan 头部，我们最关心的字段 VNI 确实是 444&lt;/li&gt;
&lt;li&gt;最外层（图中最上面）是 vtep 所在主机的通信报文头部。可以看到这里 IP 地址为多播 &lt;code&gt;239.1.1.1&lt;/code&gt;，目的 MAC 地址也是多播对应的地址&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而 ARP 应答报文不是多播而是单播的事实也能看出来：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tKfTcgy1fjybddu0boj31kw11ekaq.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从上面的通信过程，可以看出不少信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;多播其实就相当于 vtep 之间的广播，报文会发给所有的 vtep，但是只有一个会做出应答&lt;/li&gt;
&lt;li&gt;vtep 会通过接收到的报文学习 &lt;code&gt;MAC - VNI - Vtep IP&lt;/code&gt; 的信息，减少后续不必要的多播报文&lt;/li&gt;
&lt;li&gt;对于 overlay 网络中的通信实体来说，整个通信过程对它们的透明的，它们认为自己的通信过程和经典网络没有区别&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通信结束之后，可以在主机上看到保存的 ARP 缓存：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip neigh |grep vxlan1
172.55.1.2 dev vxlan1 lladdr 0a:86:31:3c:0d:6b STALE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外查看bridge的fdb信息也有所变化：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# bridge fdb | grep vxlan1
00:00:00:00:00:00 dev vxlan1 dst 239.1.1.1 via eth0 self permanent
0a:86:31:3c:0d:6b dev vxlan1 dst 192.168.1.100 self 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，增加了一条点对点的信息。&lt;/p&gt;

&lt;p&gt;上述我们演示了点对点的vxlan，以及多播模式的vxlan，但是测试都是通过vtep设备的ip来进行通信的，下面使用linux bridge以及namespace来模拟docker跨宿主机之间使用overlay（vxlan）通信过程。&lt;/p&gt;

&lt;h3 id=&#34;利用bridge接入容器&#34;&gt;利用bridge接入容器&lt;/h3&gt;

&lt;p&gt;​   尽管上面的方法能够通过多播实现自动化的 overlay 网络构建，但是通信的双方只有 vtep，在实际的生产中，每台主机上都有几十台甚至上百台的虚拟机或者容器需要通信，因此我们需要找到一种方法能够把这些通信实体组织起来。&lt;/p&gt;

&lt;p&gt;​   在 linux 中把同一个网段的 interface 组织起来正是网桥（bridge，或者 switch，这两个名称等价）的功能，因此这部分我们介绍如何用网桥把多个虚拟机或者容器放到同一个 vxlan overlay 网络中。最终实现的网络架构如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://tva1.sinaimg.cn/large/007X8olVly1g6wr1rccnzj30fl0aw3yn.jpg&#34; alt=&#34;1566871032525&#34; /&gt;&lt;/p&gt;

&lt;p&gt;​   因为创建虚拟机或者容器比较麻烦，我们用 network namespace 来模拟，从理论上它们是一样的。关于 network namespace 和 veth pair 的基础知识，请参考我&lt;a href=&#34;https://cizixs.com/2017/02/10/network-virtualization-network-namespace&#34;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;​   对于每个容器/虚拟机，我们创建一个 network namespace，并通过一对 veth pair 把容器中的 eth0 网络连接到网桥上。同时 vtep 也会放到网桥上，以便能够对报文进行 vxlan 相关的处理。&lt;/p&gt;

&lt;p&gt;​   首先我们创建 vtep interface，使用的是多播模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip link add vxlan1 type vxlan \
    id 444 \
    dstport 4789 \
    group 239.1.1.1 \
    local 192.168.1.100 \
    dev eth0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   然后创建网桥 &lt;code&gt;br0&lt;/code&gt;，把 vtep interface 绑定到上面：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 创建名字为br0的网桥
# ip link add br0 type bridge
# 将vxlan设备vxlan1加到br0上
# ip link set dev vxlan1 master br0
# ip link set vxlan1 up
# ip link set br0 up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   下面使用network namespace来模拟container，利用veth pair设备将容器桥接到br0上：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip netns add container1

# 创建 veth pair，并把一端加到网桥上
# ip link add veth0 type veth peer name veth1
# ip link set dev veth0 master br0
# ip link set dev veth0 up

# 配置容器内部的网络和 IP
# ip link set dev veth1 netns container1
# ip netns exec container1 ip link set lo up

# ip netns exec container1 ip link set veth1 name eth0
# ip netns exec container1 ip addr add 172.55.1.2/24 dev eth0
# ip netns exec container1 ip link set eth0 up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   这样子在主机10.104.109.48就设置好了vxlan设备和容器，可以查看具体的设备情况：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# brctl show br0
bridge name     bridge id               STP enabled     interfaces
br0             8000.0a86313c0d6b       no              veth0
                                                        vxlan1

# ifconfig vxlan1
vxlan1: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1400
        inet6 fe80::886:31ff:fe3c:d6b  prefixlen 64  scopeid 0x20&amp;lt;link&amp;gt;
        ether 0a:86:31:3c:0d:6b  txqueuelen 0  (Ethernet)
        RX packets 13  bytes 732 (732.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 13  bytes 1122 (1.0 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
# ip netns exec container1 ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
1042: eth0@if1043: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether c6:5d:76:b7:df:24 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.55.1.2/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::c45d:76ff:feb7:df24/64 scope link 
       valid_lft forever preferred_lft forever
# ip netns exec container1 ip r
172.55.1.0/24 dev eth0  proto kernel  scope link  src 172.55.1.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述步骤在另外一台机器上执行，然后验证网络连通性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip netns exec container1 ping -c 4 172.55.1.2
PING 172.55.1.2 (172.55.1.2) 56(84) bytes of data.
64 bytes from 172.55.1.2: icmp_seq=1 ttl=64 time=0.536 ms
64 bytes from 172.55.1.2: icmp_seq=2 ttl=64 time=0.540 ms
64 bytes from 172.55.1.2: icmp_seq=3 ttl=64 time=0.673 ms
64 bytes from 172.55.1.2: icmp_seq=4 ttl=64 time=2.82 ms

--- 172.55.1.2 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3000ms
rtt min/avg/max/mdev = 0.536/1.142/2.820/0.970 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;容器通信过程和前面的实验类似，只不过这里容器发出的 ARP 报文会被网桥转发给 &lt;code&gt;vxlan0&lt;/code&gt;，然后 &lt;code&gt;vxlan0&lt;/code&gt; 添加 vxlan 头部通过多播来找到对方的 MAC 地址。&lt;/p&gt;

&lt;p&gt;从逻辑上可以认为，在 &lt;code&gt;vxlan1&lt;/code&gt; 的帮助下同一个 vxlan overlay 网络中的容器是连接到同一个网桥上的，示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://tva1.sinaimg.cn/large/007X8olVly1g6wr243umcj30fl0awt8w.jpg&#34; alt=&#34;1566871334979&#34; /&gt;&lt;/p&gt;

&lt;p&gt;多播实现很简单，不需要中心化的控制。但是不是所有的网络都支持多播，而且需要事先规划多播组和 VNI 的对应关系，在 overlay 网络数量比较多时也会很麻烦，多播也会导致大量的无用报文在网络中出现。现在很多云计算的网络都会通过自动化的方式来发现 vtep 和 MAC 信息，也就是自动构建 vxlan 网络。下面的几个部分，我们来解开自动化 vxlan 网络的秘密。&lt;/p&gt;

&lt;h3 id=&#34;手动维护-vtep-组&#34;&gt;手动维护 vtep 组&lt;/h3&gt;

&lt;p&gt;经过上面几个实验，我们来思考一下为什么要使用多播。因为对 overlay 网络来说，它的网段范围是分布在多个主机上的，因此传统 ARP 报文的广播无法直接使用。要想做到 overlay 网络的广播，必须把报文发送到所有 vtep 在的节点，这才引入了多播。&lt;/p&gt;

&lt;p&gt;如果有一种方法能够不通过多播，能把 overlay 的广播报文发送给所有的 vtep 主机的话，也能达到相同的功能。当然在维护 vtep 网络组之前，必须提前知道哪些 vtep 要组成一个网络，以及这些 vtep 在哪些主机上。&lt;/p&gt;

&lt;p&gt;Linux 的 vxlan 模块也提供了这个功能，而且实现起来并不复杂。创建 vtep interface 的时候不使用 &lt;code&gt;remote&lt;/code&gt; 或者 &lt;code&gt;group&lt;/code&gt; 参数就行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip link add vxlan0 type vxlan \
    id 42 \
    dstport 4789 \
    dev eth0 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 vtep interface 创建的时候没有指定多播地址，当第一个 ARP 请求报文发送时它也不知道要发送给谁。但是我们可以手动添加默认的 FDB 表项，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ bridge fdb append 00:00:00:00:00:00 dev vxlan0 dst 192.168.1.100
$ bridge fdb append 00:00:00:00:00:00 dev vxlan0 dst 192.168.1.200

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样的话，如果不知道对方 VTEP 的地址，就会往选择默认的表项，发到 &lt;code&gt;192.168.8.100&lt;/code&gt; 和 &lt;code&gt;192.168.8.200&lt;/code&gt;，相当于手动维护了一个 vtep 的多播组。&lt;/p&gt;

&lt;p&gt;在所有的节点的 vtep 上更新对应的 fdb 表项，就能实现 overlay 网络的连通。整个通信流程和多播模式相同，唯一的区别是，vtep 第一次会给所有的组内成员发送单播报文，当然也只有一个 vtep 会做出应答。&lt;/p&gt;

&lt;p&gt;使用一些自动化工具，定时更新 FDB 表项，就能动态地维护 VTEP 的拓扑结构。&lt;/p&gt;

&lt;p&gt;这个方案解决了在某些 underlay 网络中不能使用多播的问题，但是并没有解决多播的另外一个问题：每次要查找 MAC 地址要发送大量的无用报文，如果 vtep 组节点数量很大，那么每次查询都发送 N 个报文，其中只有一个报文真正有用。&lt;/p&gt;

&lt;h3 id=&#34;手动维护fdb表&#34;&gt;手动维护fdb表&lt;/h3&gt;

&lt;p&gt;如果提前知道目的容器 MAC 地址和它所在主机的 IP 地址，也可以通过更新 fdb 表项来减少广播的报文数量。&lt;/p&gt;

&lt;p&gt;创建 vtep 的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip link add vxlan0 type vxlan \
    id 42 \
    dstport 4789 \
    dev eth0 \
    nolearning

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这次我们添加了 &lt;code&gt;nolearning&lt;/code&gt; 参数，这个参数告诉 vtep 不要通过收到的报文来学习 fdb 表项的内容，因为我们会自动维护这个列表。&lt;/p&gt;

&lt;p&gt;然后可以添加 fdb 表项告诉 vtep 容器 MAC 对应的主机 IP 地址：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ bridge fdb append 52:5e:55:58:9a:ab dev vxlan0 dst 192.168.1.100
$ bridge fdb append d6:d9:cd:0a:a4:28 dev vxlan0 dst 192.168.1.200

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果知道了对方的 MAC 地址，vtep 搜索 fdb 表项就知道应该发送到哪个对应的 vtep 了。需要注意是的，这个情况还是需要默认的表项（那些全零的表项），在不知道容器 IP 和 MAC 对应关系的时候通过默认方式发送 ARP 报文去查询对方的 MAC 地址。&lt;/p&gt;

&lt;p&gt;需要注意的是，和上一个方法相比，这个方法并没有任何效率上的改进，只是把自动学习 fdb 表项换成了手动维护（当然实际情况一般是自动化程序来维护），第一次发送 ARP 请求还是会往 vtep 组发送大量单播报文。&lt;/p&gt;

&lt;p&gt;当时这个方法给了我们很重要的提示：如果实现知道 vxlan 网络的信息，&lt;strong&gt;vtep 需要的信息都是可以自动维护的，而不需要学习&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&#34;手动维护arp表&#34;&gt;手动维护arp表&lt;/h3&gt;

&lt;p&gt;除了维护 fdb 表，arp 表也是可以维护的。如果能通过某个方式知道容器的 IP 和 MAC 地址对应关系，只要更新到每个节点，就能实现网络的连通。&lt;/p&gt;

&lt;p&gt;但是这里有个问题，我们需要维护的是每个容器里面的 ARP 表项，因为最终通信的双方是容器。到每个容器里面（所有的 network namespace）去更新对应的 ARP 表，是件工作量很大的事情，而且容器的创建和删除还是动态的，。linux 提供了一个解决方案，vtep 可以作为 arp 代理，回复 arp 请求，也就是说只要 vtep interface 知道对应的 &lt;code&gt;IP - MAC&lt;/code&gt; 关系，在接收到容器发来的 ARP 请求时可以直接作出应答。这样的话，我们只需要更新 vtep interface 上 ARP 表项就行了。&lt;/p&gt;

&lt;p&gt;创建 vtep interface 需要加上 &lt;code&gt;proxy&lt;/code&gt; 参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip link add vxlan0 type vxlan \
    id 42 \
    dstport 4789 \
    dev eth0 \
    nolearning \
    proxy

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这条命令和上部分相比多了 &lt;code&gt;proxy&lt;/code&gt; 参数，这个参数告诉 vtep 承担 ARP 代理的功能。如果收到 ARP 请求，并且自己知道结果就直接作出应答。&lt;/p&gt;

&lt;p&gt;当然我们还是要手动更新 fdb 表项来构建 vtep 组，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ bridge fdb append 52:5e:55:58:9a:ab dev vxlan0 dst 192.168.1.100
$ bridge fdb append d6:d9:cd:0a:a4:28 dev vxlan0 dst 192.168.1.200

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，还需要为 vtep 添加 arp 表项，所有要通信容器的 &lt;code&gt;IP - MAC&lt;/code&gt;二元组都要加进去。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip neigh add 172.55.1.2 lladdr d6:d9:cd:0a:a4:28 dev vxlan0
$ ip neigh add 172.55.1.3 lladdr 52:5e:55:58:9a:ab dev vxlan0

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在要通信的所有节点配置完之后，容器就能互相 ping 通。当容器要访问彼此，并且第一次发送 ARP 请求时，这个请求并不会发给所有的 vtep，而是当前由当前 vtep 做出应答，大大减少了网络上的报文。&lt;/p&gt;

&lt;p&gt;借助自动化的工具做到实时的表项（fdb 和 arp）更新，这种方法就能很高效地实现 overlay 网络的通信。&lt;/p&gt;

&lt;h3 id=&#34;动态更新-arp-和-fdb-表项&#34;&gt;动态更新 arp 和 fdb 表项&lt;/h3&gt;

&lt;p&gt;尽管前一种方法通过动态更新 fdb 和 arp 表避免多余的网络报文，但是还有一个的问题：为了能够让所有的容器正常工作，所有可能会通信的容器都必须提前添加到 ARP 和 fdb 表项中。但并不是网络上所有的容器都会互相通信，所以&lt;strong&gt;添加的有些表项（尤其是 ARP 表项）是用不到的&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Linux 提供了另外一种方法，内核能够动态地通知节点要和哪个容器通信，应用程序可以订阅这些事件，如果内核发现需要的 ARP 或者 fdb 表项不存在，会发送事件给订阅的应用程序，这样应用程序从中心化的控制拿到这些信息来更新表项，做到更精确的控制。&lt;/p&gt;

&lt;p&gt;要收到 L2（fdb）miss，必须要满足几个条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;目的 MAC 地址未知，也就是没有对应的 fdb 表项&lt;/li&gt;
&lt;li&gt;fdb 中没有全零的表项，也就是说默认规则&lt;/li&gt;
&lt;li&gt;目的 MAC 地址不是多播或者广播地址&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要实现这种功能，创建 vtep 的时候需要加上额外的参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip link add vxlan0 type vxlan \
    id 42 \
    dstport 4789 \
    dev eth0 \
    nolearning \
    proxy \
    l2miss \
    l3miss

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这次多了两个参数 &lt;code&gt;l2miss&lt;/code&gt; 和 &lt;code&gt;l3miss&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;l2miss&lt;/code&gt;：如果设备找不到 MAC 地址需要的 vtep 地址，就发送通知事件&lt;/li&gt;
&lt;li&gt;&lt;code&gt;l3miss&lt;/code&gt;：如果设备找不到需要 IP 对应的 MAC 地址，就发送通知事件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;ip monitor&lt;/code&gt; 命令能做到这点，监听某个 interface 的事件，具体用法请参考 man 手册.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# ip monitor all dev vxlan0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如果从本节点容器 ping 另外一个节点的容器，就先发生 l3 miss，这是 l3miss 的通知事件，：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip monitor all dev vxlan0
[nsid current]miss 172.55.1.3  STALE

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;l3miss&lt;/code&gt; 是说这个 IP 地址，vtep 不知道它对应的 MAC 地址，因此要手动添加 arp 记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip neigh replace 172.55.1.3 \
    lladdr b2:ee:aa:42:8b:0b \
    dev vxlan0 \
    nud reachable

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这条命令设置的 &lt;code&gt;nud reachable&lt;/code&gt; 参数意思是，这条记录有一个超时时间，系统发现它无效一段时间会自动删除。这样的好处是，不需要手动去删除它，删除后需要通信内核会再次发送通知事件。 &lt;code&gt;nud&lt;/code&gt; 是 &lt;code&gt;Neighbour Unreachability Detection&lt;/code&gt; 的缩写， 当然根据需要这个参数也可以设置成其他值，比如 &lt;code&gt;permanent&lt;/code&gt;，表示这个记录永远不会过时，系统不会检查它是否正确，也不会删除它，只有管理员也能对它进行修改。&lt;/p&gt;

&lt;p&gt;这时候还是不能正常通信，接着会出现 l2miss 的通知事件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ip monitor all dev vxlan0
[nsid current]miss lladdr b2:ee:aa:42:8b:0b STALE

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;类似的，这个事件是说不知道这个容器的 MAC 地址在哪个节点上，所以要手动添加 fdb 记录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# bridge fdb add b2:ee:aa:42:8b:0b dst 192.168.8.101 dev vxlan0

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在通信的另一台机器上执行响应的操作，就会发现两者能 ping 通了。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;上面提出的所有方案中，其中手动的部分都可以使用程序来自动完成，需要的信息一般都是从集中式的控制中心获取的，这也是大多数基于 vxlan 的 SDN 网络的大致架构。当然具体的实现不一定和某种方法相同，可能是上述方法的变形或者组合，但是设计思想都是一样的。&lt;/p&gt;

&lt;p&gt;虽然上述的实验中，为了简化图中只有两台主机，而且只有一个 vxlan 网络，但是利用相同的操作很容易创建另外一个 vxlan 网络（必须要保证 vtep 的 VNI 值不同，如果使用多播，也要保证多播 IP 不同），如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://tva1.sinaimg.cn/large/007X8olVly1g6wr2luuenj30pg0b2jrq.jpg&#34; alt=&#34;1566871782602&#34; /&gt;&lt;/p&gt;

&lt;p&gt;主机会根据 VNI 来区别不同的 vxlan 网络，不同的 vxlan 网络之间不会相互影响。如果再加上 network namespace，就能实现更复杂的网络结构。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>使用hugo搭建博客</title>
      <link>http://qsyqian.github.io/post/use-hugo-deploy-blog/</link>
      <pubDate>Wed, 14 Aug 2019 10:50:41 +0800</pubDate>
      
      <guid>http://qsyqian.github.io/post/use-hugo-deploy-blog/</guid>
      
        <description>

&lt;h1 id=&#34;使用hugo和github搭建博客&#34;&gt;使用hugo和github搭建博客&lt;/h1&gt;

&lt;h2 id=&#34;一-安装hugo&#34;&gt;一、安装hugo&lt;/h2&gt;

&lt;p&gt;​   去&lt;a href=&#34;https://github.com/gohugoio/hugo/releases&#34;&gt;hugo下载地址&lt;/a&gt;下载所需要的版本的hugo软件，如果是windows的就下载对应的压缩包，解压后得到&lt;code&gt;hugo.exe&lt;/code&gt;，然后配置好环境变量，就可以在cmd中全局使用hugo命令。&lt;/p&gt;

&lt;h2 id=&#34;二-创建本地博客目录&#34;&gt;二、创建本地博客目录&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd d:
hugo.exe new site sqian-blog
cd sqian-blog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   上述三条命令分别是：进入到D盘目录，使用hugo创建一个博客目录，进入到该目录中。目录结构大概是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt; ll
total 25
drwxr-xr-x 1 user 197121    0 8月  12 18:25 archetypes/
-rw-r--r-- 1 user 197121 4283 8月  14 09:42 config.toml
-rw-r--r-- 1 user 197121  842 8月  12 18:55 config.toml.casper
drwxr-xr-x 1 user 197121    0 8月  12 18:36 content/
drwxr-xr-x 1 user 197121    0 8月  12 18:25 data/
drwxr-xr-x 1 user 197121    0 8月  12 18:25 layouts/
drwxr-xr-x 1 user 197121    0 8月  31  1754 public/
-rw-r--r-- 1 user 197121   40 8月  12 19:24 README.md
drwxr-xr-x 1 user 197121    0 8月  12 18:28 resources/
drwxr-xr-x 1 user 197121    0 8月  12 18:25 static/
drwxr-xr-x 1 user 197121    0 8月  14 09:36 themes/

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;三-创建文章&#34;&gt;三、创建文章&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hugo new about.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   上述命令使用hugo来创建一个名字为&lt;code&gt;about.md&lt;/code&gt;的文章。执行完上述命令之后，在content目录下回生成该文件。&lt;/p&gt;

&lt;p&gt;​   当然也可以给文章分类，创建到不同的目录中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hugo new post/first.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   上述命令在content的post目录下，生成名字为&lt;code&gt;first.md&lt;/code&gt;的文章。&lt;/p&gt;

&lt;p&gt;​   打开上述md文件，编辑保存即可。&lt;/p&gt;

&lt;h2 id=&#34;四-安装主题&#34;&gt;四、安装主题&lt;/h2&gt;

&lt;p&gt;​   hugo支持多种主题，在github上也有很多开源的主题可供使用。&lt;/p&gt;

&lt;p&gt;​   本次我们选择一个简约的主题，地址为：&lt;a href=&#34;https://github.com/xianmin/hugo-theme-jane&#34;&gt;https://github.com/xianmin/hugo-theme-jane&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;​   不同的主题安装过程大体都类似，比如安装上述主题的过程为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;下载主题到主题目录：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;git clone https://github.com/xianmin/hugo-theme-jane.git --depth=1 themes/jane&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;拷贝并编辑配置文件：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果细心点会发现在我们创建的博客根目录下有个配置文件&lt;code&gt;config.yaml&lt;/code&gt;，该配置文件根据不同的主题，配置项可能不同，我们直接从该主题中拷贝一个样本配置文件到博客根目录，然后根据需要修改：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cp themes/jane/exampleSite/config.toml ./&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;五-本地预览&#34;&gt;五、本地预览&lt;/h2&gt;

&lt;p&gt;​   上述我们编辑了我们的博客内容，修改了主题，需要现在本地预览，看是否有书写、格式错误，等待无误后再发布出去。&lt;/p&gt;

&lt;p&gt;​   可以使用hugo在本地直接启动server来预览，命令如下：&lt;/p&gt;

&lt;p&gt;​   &lt;code&gt;hugo server --theme=hyde --buildDrafts&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;​   上述有两个参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ndash;theme：指定渲染的主题；&lt;/li&gt;
&lt;li&gt;&amp;ndash;buildDrafts：使用hugo自动生成的md博客文件头中，都会有对应的draft参数，表示该文章还在编辑中。预览模式下我们期望的预览所有，所以加上该参数。如果我们博客编辑完成了，需要把改参数去掉，然后使用hugo编译，发布到github上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​   hugo本地启动之后，就可以浏览器访问&lt;code&gt;localhost:1313&lt;/code&gt;来预览效果，如果有问题，及时修改即可。&lt;/p&gt;

&lt;h2 id=&#34;六-发布&#34;&gt;六、发布&lt;/h2&gt;

&lt;p&gt;​   上述我们在本地已经调试好了。如果将我们的博客发布到github上给别人看呢。这里需要普及的一个知识是，每一个github账户可以创建一个特殊的仓库：&lt;code&gt;$username.github.io&lt;/code&gt;，创建好之后，我们可以通过浏览器访问地址:&lt;code&gt;http://$username.github.io&lt;/code&gt;来访问主页。因此我们可以在该特殊的仓库中建立我们的博客站点。&lt;a href=&#34;https://pages.github.com/&#34;&gt;github Pages&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;​   假设我们创建好仓库&lt;code&gt;https://github.com/qsyqian/qsyqian.github.io&lt;/code&gt;。进入到我们的站点根目录，编译生成站点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hugo --theme=jane
Building sites …
                   | EN
+------------------+----+
  Pages            | 13
  Paginator pages  |  0
  Non-page files   |  0
  Static files     | 19
  Processed images |  0
  Aliases          |  2
  Sitemaps         |  1
  Cleaned          |  0

Total in 188 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   主题选项要选择自己已经安装好的主题。&lt;/p&gt;

&lt;p&gt;​   编译好之后，在./public目录下会生成站点的静态文件，我们要做的就是把./public目录和我们在github创建的仓库连接起来：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd public
$ git init
$ git remote add origin https://github.com/qsyqian/qsyqian.github.io.git
$ git add -A
$ git commit -m &amp;quot;first commit&amp;quot;
$ git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​   上述工作全部做完之后，大概过1-2分钟之后，访问自己的博客站点&lt;code&gt;qsyqian.github.io&lt;/code&gt;就可以看到自己的博客站点啦。撒花~&lt;/p&gt;

&lt;h2 id=&#34;七-参考链接&#34;&gt;七、参考链接&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.gohugo.org/&#34;&gt;https://www.gohugo.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gohugoio/hugo/releases&#34;&gt;https://github.com/gohugoio/hugo/releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/266175192&#34;&gt;https://www.zhihu.com/question/266175192&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/xianmin/hugo-theme-jane&#34;&gt;https://github.com/xianmin/hugo-theme-jane&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pages.github.com/&#34;&gt;https://pages.github.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>About</title>
      <link>http://qsyqian.github.io/about/</link>
      <pubDate>Mon, 12 Aug 2019 18:28:13 +0800</pubDate>
      
      <guid>http://qsyqian.github.io/about/</guid>
      
        <description>

&lt;h2 id=&#34;about&#34;&gt;About&lt;/h2&gt;
</description>
      
    </item>
    
  </channel>
</rss>
