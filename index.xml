<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>qianshuangyang</title>
    <link>https://qsyqian.github.io/</link>
    <description>Recent content on qianshuangyang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Nov 2019 09:41:10 +0800</lastBuildDate>
    
	<atom:link href="https://qsyqian.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ovs中debug数据包流向</title>
      <link>https://qsyqian.github.io/post/ovs-trace-usage/</link>
      <pubDate>Thu, 14 Nov 2019 09:41:10 +0800</pubDate>
      
      <guid>https://qsyqian.github.io/post/ovs-trace-usage/</guid>
      <description>ovs中debug数据包流向 ovs是一个可编程软件交换机，因此它可以在数据包级别做动作。本文主要介绍了如何使用trace工具来跟踪数据包在datapath中的流动路径。
数据包跟踪 为了更好地理解该工具，使用下述流表来举例：
table=3,ip,tcp,tcp_dst=80,action=output:2 table=2,ip,tcp,tcp_dst=22,action=output:1 table=0,in_port=3,ip,nw_src=192.0.2.0/24,action=resubmit(,2) table=0,in_port=3,ip,nw_src=198.51.100.0/24,action=resubmit(,3)  第一行定义了在table=3中的，tcp目的端口为80的匹配，如果能匹配成功，则转发到端口2。
第二行定义了在table=2中的，tcp目的端口为22的匹配，如果匹配成功，则转发到端口1。
其余两行定义了源IP的匹配规则，如果能匹配成功，则通过resubmit转发到对应的table上。
现在来看下，假如一个数据包的源IP为192.0.2.1，目的端口为22，从端口1流入，该怎么通过上述流表：
$ ovs-appctl ofproto/trace br0 in_port=3,tcp,nw_src=192.0.2.2,tcp_dst=22 Flow: tcp,in_port=3,vlan_tci=0x0000,dl_src=00:00:00:00:00:00,dl_dst=00:00:00:00:00:00,nw_src=192.0.2.2,nw_dst=0.0.0.0,nw_tos=0,nw_ecn=0,nw_ttl=0,tp_src=0,tp_dst=22,tcp_flags=0 bridge(&amp;quot;br0&amp;quot;) ------------- 0. ip,in_port=3,nw_src=192.0.2.0/24, priority 32768 resubmit(,2) 2. tcp,tp_dst=22, priority 32768 output:1 Final flow: unchanged Megaflow: recirc_id=0,tcp,in_port=3,nw_src=192.0.2.0/24,nw_frag=no,tp_dst=22 Datapath actions: 1  第一行是跟踪的命令。br0是数据包要通过的网桥。剩下的参数就是描述数据包本身。比如说，nw_src匹配数据包的源IP。所有的参数可参见文档ovs-fields。
第二行是从第一行提取出来的流。未指定的参数都设置为0。
第二部分就是数据包在br0中流通的过程。可以看到，在表0中，匹配到了对应的数据包，并且给出了对应的action（resubmit到了表2中）。然后以数字2开头的，表示数据包在表2中的流向过程，由于匹配到了目的端口为22，因此将数据包转发到端口1里。
总的来说，可以跟踪数据包整个的流向直到最后从某个端口转发出去或者drop掉。最后，跟踪工具给出了Megafkow，来匹配数据包的所有的字段。
下面来看同样的数据包但是目的端口不同，是如何转发的。
$ ovs-appctl ofproto/trace br0 in_port=3,tcp,nw_src=192.0.2.2,tcp_dst=80 Flow: tcp,in_port=3,vlan_tci=0x0000,dl_src=00:00:00:00:00:00,dl_dst=00:00:00:00:00:00,nw_src=192.0.2.2,nw_dst=0.0.0.0,nw_tos=0,nw_ecn=0,nw_ttl=0,tp_src=0,tp_dst=80,tcp_flags=0 bridge(&amp;quot;br0&amp;quot;) ------------- 0. ip,in_port=3,nw_src=192.0.2.0/24, priority 32768 resubmit(,2) 2. No match. drop Final flow: unchanged Megaflow: recirc_id=0,tcp,in_port=3,nw_src=192.0.2.0/24,nw_frag=no,tp_dst=0x40/0xffc0 Datapath actions: drop  在数据匹配那一部分可以看到，数据包在table=0匹配到，是因为源IP匹配，因此转发给table=2 。在table=2中没有匹配到一条规则。这种情况称之为table miss。根据使用的ovs的版本不一样，出现table miss的时候的action也不一样。上述例子中，默认动作为drop。</description>
    </item>
    
    <item>
      <title>K8s 从懵圈到熟练 – 集群网络详解【转载阿里云公众号】</title>
      <link>https://qsyqian.github.io/post/k8s-cluster-network/</link>
      <pubDate>Mon, 07 Oct 2019 14:38:01 +0800</pubDate>
      
      <guid>https://qsyqian.github.io/post/k8s-cluster-network/</guid>
      <description>K8s 从懵圈到熟练 – 集群网络详解 导读： 阿里云 K8S 集群网络目前有两种方案：一种是 flannel 方案；另外一种是基于 calico 和弹性网卡 eni 的 terway 方案。Terway 和 flannel 类似，不同的地方在于 terway 支持 Pod 弹性网卡，以及 NetworkPolicy 功能。本文中，作者基于当前的 1.12.6 版本，以 flannel 为例，深入分析阿里云 K8S 集群网络的实现方法。
### 鸟瞰
总体上来说，阿里云 K8S 集群网络配置完成之后，如下图所示：包括
 集群 CIDR VPC 路由表 节点网络 节点的 podCIDR 节点上的虚拟网桥 cni0 连接 Pod 和网桥的 veth  类似的图大家可能在很多文章中都看过，但因为其中相关配置过于复杂，比较难理解。这里我们可以看下这些配置背后的逻辑。
基本上我们可以把这些配置分三种情况来理解：集群配置，节点配置以及 Pod 配置。与这三种情况对应的，其实是对集群网络 IP 段的三次划分：首先是集群 CIDR，接着是为每个节点分配 podCIDR（即集群 CIDR 的子网段），最后在 podCIDR 里为每个 Pod 分配自己的 IP。
集群网络创建 初始阶段 集群的创建，基于云资源 VPC 和 ECS，在创建完 VPC 和 ECS 之后，我们基本上可以得到如下图的资源配置。我们得到一个 VPC，这个 VPC 的网段是 192.</description>
    </item>
    
    <item>
      <title>Flannel 模式网络详解（vxlan）</title>
      <link>https://qsyqian.github.io/post/flannel-pod-communication-analysis/</link>
      <pubDate>Tue, 17 Sep 2019 15:08:18 +0800</pubDate>
      
      <guid>https://qsyqian.github.io/post/flannel-pod-communication-analysis/</guid>
      <description>Flannel 模式网络详解（vxlan） 本文主要分析vxlan作为backend的flannel，如何实现跨宿主机通信。
相关的flannel原理介绍的博客很多了，我就不分析原理，主要是介绍flannel安装完成之后，宿主机上的网络设备，以及跨宿主机的pod是如何实现一步步通信的。
环境介绍 本文使用的是三个节点的k8s，版本是1.13 。使用的flannel版本是0.10.0 。flannel配置的backend是vxlan。如下：
# kubectl get nodes NAME STATUS ROLES AGE VERSION sqian-k8s-node1 Ready master,node 4h53m v1.13.0 sqian-k8s-node2 Ready master,node 4h52m v1.13.0 sqian-k8s-node3 Ready master,node 4h52m v1.13.0 # kubectl exec -it kube-flannel-4rrms -n kube-system -- /opt/bin/flanneld -version Defaulting container name to kube-flannel. Use &#39;kubectl describe pod/kube-flannel-4rrms -n kube-system&#39; to see all of the containers in this pod. v0.10.0  宿主机网络情况 与flannel相关的几个虚拟网络上设备：
 flannel.1：这是一个vxlan设备。也就是耳熟能详的vteh设备，负责网络数据包的封包和解封。 cni0：是一个linux bridge，用于连接同一个宿主机上的pod。 vethf12090da@if3：容器内eth0网卡的对端设备，从名字上看，在容器内eth0网卡的编号应为3。  下面再看下上述网络设备的网络信息：</description>
    </item>
    
    <item>
      <title>Flannel源码分析</title>
      <link>https://qsyqian.github.io/post/flannel-source-code-analysis/</link>
      <pubDate>Thu, 12 Sep 2019 16:59:05 +0800</pubDate>
      
      <guid>https://qsyqian.github.io/post/flannel-source-code-analysis/</guid>
      <description>Flannel源码分析 前言 flannel作为kubernetes的一种网络解决方案，在社区是比较活跃的。支持多种backend。
flannel源码地址在：https://github.com/coreos/flannel
从官网上把flannel的代码clone下来，目录结构如下：
$ ll -ah total 222K drwxr-xr-x 1 user 197121 0 8月 30 18:37 ./ drwxr-xr-x 1 user 197121 0 8月 30 18:31 ../ -rw-r--r-- 1 user 197121 351 8月 30 18:33 .appveyor.yml -rw-r--r-- 1 user 197121 56 8月 30 18:33 .dockerignore drwxr-xr-x 1 user 197121 0 9月 17 09:34 .git/ drwxr-xr-x 1 user 197121 0 8月 30 18:33 .github/ -rw-r--r-- 1 user 197121 147 8月 30 18:33 .</description>
    </item>
    
    <item>
      <title>手动创建vxlan网络连接container</title>
      <link>https://qsyqian.github.io/post/linux-vxlan-implement/</link>
      <pubDate>Tue, 27 Aug 2019 10:57:51 +0800</pubDate>
      
      <guid>https://qsyqian.github.io/post/linux-vxlan-implement/</guid>
      <description>手动创建vxlan网络连接container ​ 本文主要对linux下实现vxlan网络有个初步的认识，涉及较多的是动手操作，而不是理论知识。如果想知道更多关于vxlan协议的一些理论，包括产生的北京，报文的格式，请参考vxlan 协议的介绍文章。
​ 在下述内容中我们主要是在linux上模拟vxlan设备，实现docker跨主机的overlay网络。
实验环境 本次实验我们使用两个linux机器，系统为centos7，IP信息如下：
 192.168.1.100 192.168.1.200  ​ 我们搭建的overlay网络的网段为172.55.1.0/24 。实验目的就是vxlan能够通过overlay IP互相连接。
实验 点对点的vxlan 先来搭建一个最简单的 vxlan 网络，两台机器构成一个 vxlan 网络，每台机器上有一个 vtep，vtep 通过它们的 IP 互相通信。这次实验完成后的网络结构如下图所示：  首先使用 ip 命令创建我们的 vxlan interface，在192.168.1.100上执行下述命令：
ip link add vxlan1 type vxlan id 444 dstport 4789 remote 192.168.1.200 local 192.168.1.100 dev eth0
大概解释下上述参数的含义：
 vxlan1：所创建的网络设备的名字 type vxlan：所创建的网络设备的类型 id 444：所创建vxlan VNI，这个值可以在 1 到 2^24 之间 dstport 4789:vtep 通信的端口，linux 默认使用 8472（为了保持兼容，默认值一直没有更改），而 IANA 分配的端口是 4789，所以我们这里显式指定了它的值 remote 192.</description>
    </item>
    
    <item>
      <title>使用hugo搭建博客</title>
      <link>https://qsyqian.github.io/post/use-hugo-deploy-blog/</link>
      <pubDate>Wed, 14 Aug 2019 10:50:41 +0800</pubDate>
      
      <guid>https://qsyqian.github.io/post/use-hugo-deploy-blog/</guid>
      <description>使用hugo和github搭建博客 一、安装hugo ​ 去hugo下载地址下载所需要的版本的hugo软件，如果是windows的就下载对应的压缩包，解压后得到hugo.exe，然后配置好环境变量，就可以在cmd中全局使用hugo命令。
二、创建本地博客目录 cd d: hugo.exe new site sqian-blog cd sqian-blog  ​ 上述三条命令分别是：进入到D盘目录，使用hugo创建一个博客目录，进入到该目录中。目录结构大概是：
ll total 25 drwxr-xr-x 1 user 197121 0 8月 12 18:25 archetypes/ -rw-r--r-- 1 user 197121 4283 8月 14 09:42 config.toml -rw-r--r-- 1 user 197121 842 8月 12 18:55 config.toml.casper drwxr-xr-x 1 user 197121 0 8月 12 18:36 content/ drwxr-xr-x 1 user 197121 0 8月 12 18:25 data/ drwxr-xr-x 1 user 197121 0 8月 12 18:25 layouts/ drwxr-xr-x 1 user 197121 0 8月 31 1754 public/ -rw-r--r-- 1 user 197121 40 8月 12 19:24 README.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://qsyqian.github.io/about/</link>
      <pubDate>Mon, 12 Aug 2019 18:28:13 +0800</pubDate>
      
      <guid>https://qsyqian.github.io/about/</guid>
      <description> About </description>
    </item>
    
  </channel>
</rss>