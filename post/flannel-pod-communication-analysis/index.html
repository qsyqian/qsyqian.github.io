<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>qianshuangyang  | Flannel 模式网络详解（vxlan）</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.59.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Flannel 模式网络详解（vxlan）" />
<meta property="og:description" content="Flannel 模式网络详解（vxlan） 本文主要分析vxlan作为backend的flannel，如何实现跨宿主机通信。
相关的flannel原理介绍的博客很多了，我就不分析原理，主要是介绍flannel安装完成之后，宿主机上的网络设备，以及跨宿主机的pod是如何实现一步步通信的。
环境介绍 本文使用的是三个节点的k8s，版本是1.13 。使用的flannel版本是0.10.0 。flannel配置的backend是vxlan。如下：
# kubectl get nodes NAME STATUS ROLES AGE VERSION sqian-k8s-node1 Ready master,node 4h53m v1.13.0 sqian-k8s-node2 Ready master,node 4h52m v1.13.0 sqian-k8s-node3 Ready master,node 4h52m v1.13.0 # kubectl exec -it kube-flannel-4rrms -n kube-system -- /opt/bin/flanneld -version Defaulting container name to kube-flannel. Use &#39;kubectl describe pod/kube-flannel-4rrms -n kube-system&#39; to see all of the containers in this pod. v0.10.0  宿主机网络情况 与flannel相关的几个虚拟网络上设备：
 flannel.1：这是一个vxlan设备。也就是耳熟能详的vteh设备，负责网络数据包的封包和解封。 cni0：是一个linux bridge，用于连接同一个宿主机上的pod。 vethf12090da@if3：容器内eth0网卡的对端设备，从名字上看，在容器内eth0网卡的编号应为3。  下面再看下上述网络设备的网络信息：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://qsyqian.github.io/post/flannel-pod-communication-analysis/" />
<meta property="article:published_time" content="2019-09-17T15:08:18+08:00" />
<meta property="article:modified_time" content="2019-09-17T15:08:18+08:00" />
<meta itemprop="name" content="Flannel 模式网络详解（vxlan）">
<meta itemprop="description" content="Flannel 模式网络详解（vxlan） 本文主要分析vxlan作为backend的flannel，如何实现跨宿主机通信。
相关的flannel原理介绍的博客很多了，我就不分析原理，主要是介绍flannel安装完成之后，宿主机上的网络设备，以及跨宿主机的pod是如何实现一步步通信的。
环境介绍 本文使用的是三个节点的k8s，版本是1.13 。使用的flannel版本是0.10.0 。flannel配置的backend是vxlan。如下：
# kubectl get nodes NAME STATUS ROLES AGE VERSION sqian-k8s-node1 Ready master,node 4h53m v1.13.0 sqian-k8s-node2 Ready master,node 4h52m v1.13.0 sqian-k8s-node3 Ready master,node 4h52m v1.13.0 # kubectl exec -it kube-flannel-4rrms -n kube-system -- /opt/bin/flanneld -version Defaulting container name to kube-flannel. Use &#39;kubectl describe pod/kube-flannel-4rrms -n kube-system&#39; to see all of the containers in this pod. v0.10.0  宿主机网络情况 与flannel相关的几个虚拟网络上设备：
 flannel.1：这是一个vxlan设备。也就是耳熟能详的vteh设备，负责网络数据包的封包和解封。 cni0：是一个linux bridge，用于连接同一个宿主机上的pod。 vethf12090da@if3：容器内eth0网卡的对端设备，从名字上看，在容器内eth0网卡的编号应为3。  下面再看下上述网络设备的网络信息：">


<meta itemprop="datePublished" content="2019-09-17T15:08:18&#43;08:00" />
<meta itemprop="dateModified" content="2019-09-17T15:08:18&#43;08:00" />
<meta itemprop="wordCount" content="412">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Flannel 模式网络详解（vxlan）"/>
<meta name="twitter:description" content="Flannel 模式网络详解（vxlan） 本文主要分析vxlan作为backend的flannel，如何实现跨宿主机通信。
相关的flannel原理介绍的博客很多了，我就不分析原理，主要是介绍flannel安装完成之后，宿主机上的网络设备，以及跨宿主机的pod是如何实现一步步通信的。
环境介绍 本文使用的是三个节点的k8s，版本是1.13 。使用的flannel版本是0.10.0 。flannel配置的backend是vxlan。如下：
# kubectl get nodes NAME STATUS ROLES AGE VERSION sqian-k8s-node1 Ready master,node 4h53m v1.13.0 sqian-k8s-node2 Ready master,node 4h52m v1.13.0 sqian-k8s-node3 Ready master,node 4h52m v1.13.0 # kubectl exec -it kube-flannel-4rrms -n kube-system -- /opt/bin/flanneld -version Defaulting container name to kube-flannel. Use &#39;kubectl describe pod/kube-flannel-4rrms -n kube-system&#39; to see all of the containers in this pod. v0.10.0  宿主机网络情况 与flannel相关的几个虚拟网络上设备：
 flannel.1：这是一个vxlan设备。也就是耳熟能详的vteh设备，负责网络数据包的封包和解封。 cni0：是一个linux bridge，用于连接同一个宿主机上的pod。 vethf12090da@if3：容器内eth0网卡的对端设备，从名字上看，在容器内eth0网卡的编号应为3。  下面再看下上述网络设备的网络信息："/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://qsyqian.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      qianshuangyang
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Posts page">
              Posts
            </a>
          </li>
          
        </ul>
      
      












    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">Flannel 模式网络详解（vxlan）</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2019-09-17T15:08:18&#43;08:00">September 17, 2019</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">

<h2 id="flannel-模式网络详解-vxlan">Flannel 模式网络详解（vxlan）</h2>

<p>本文主要分析vxlan作为backend的flannel，如何实现跨宿主机通信。</p>

<p>相关的flannel原理介绍的博客很多了，我就不分析原理，主要是介绍flannel安装完成之后，宿主机上的网络设备，以及跨宿主机的pod是如何实现一步步通信的。</p>

<h3 id="环境介绍">环境介绍</h3>

<p>本文使用的是三个节点的k8s，版本是1.13 。使用的flannel版本是0.10.0 。flannel配置的backend是vxlan。如下：</p>

<pre><code class="language-shell"># kubectl get nodes
NAME              STATUS   ROLES         AGE     VERSION
sqian-k8s-node1   Ready    master,node   4h53m   v1.13.0
sqian-k8s-node2   Ready    master,node   4h52m   v1.13.0
sqian-k8s-node3   Ready    master,node   4h52m   v1.13.0
# kubectl exec -it kube-flannel-4rrms  -n kube-system -- /opt/bin/flanneld -version            
Defaulting container name to kube-flannel.
Use 'kubectl describe pod/kube-flannel-4rrms -n kube-system' to see all of the containers in this pod.
v0.10.0
</code></pre>

<h3 id="宿主机网络情况">宿主机网络情况</h3>

<p>与flannel相关的几个虚拟网络上设备：</p>

<ul>
<li>flannel.1：这是一个vxlan设备。也就是耳熟能详的vteh设备，负责网络数据包的封包和解封。</li>
<li>cni0：是一个linux bridge，用于连接同一个宿主机上的pod。</li>
<li>vethf12090da@if3：容器内eth0网卡的对端设备，从名字上看，在容器内eth0网卡的编号应为3。</li>
</ul>

<p>下面再看下上述网络设备的网络信息：</p>

<h4 id="flannel-1">flannel.1：</h4>

<pre><code class="language-shell"># ifconfig flannel.1
flannel.1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450
        inet 172.168.0.0  netmask 255.255.255.255  broadcast 0.0.0.0
        inet6 fe80::e010:c0ff:fe12:aa5f  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether e2:10:c0:12:aa:5f  txqueuelen 0  (Ethernet)
        RX packets 66  bytes 5544 (5.4 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 66  bytes 7524 (7.3 KiB)
        TX errors 0  dropped 8 overruns 0  carrier 0  collisions 0
</code></pre>

<p>可以看到，flannel.1上有一个IP。（配置的容器的IP段位172.168.0.0/16），由于我们查看的node1上的flannel.1设备，而node1分配的subnet是172.168.0.0/24，该信息可以从node1的yaml文件中看到（这是因为默认的flannel使用的是kube subnet manager）。vxlan网络设备的原理这里不做赘述，感兴趣的可以看另外一篇<a href="https://qsyqian.github.io/post/linux-vxlan-implement/">博客</a>。</p>

<p>查看node1的yaml文件：</p>

<pre><code class="language-shell"># kubectl get node sqian-k8s-node1 -o yaml | grep flannel
    flannel.alpha.coreos.com/backend-data: '{&quot;VtepMAC&quot;:&quot;e2:10:c0:12:aa:5f&quot;}'
    flannel.alpha.coreos.com/backend-type: vxlan
    flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;
    flannel.alpha.coreos.com/public-ip: 10.212.36.168
</code></pre>

<p>可见flannel在每个node的yaml文件中存储了下述信息：</p>

<ul>
<li>flannel.alpha.coreos.com/backend-data: vteh设备（flannel.1）的mac地址</li>
<li>flannel.alpha.coreos.com/backend-type: backend type</li>
<li>flannel.alpha.coreos.com/kube-subnet-manager: true 采用kube subnet manager</li>
<li>flannel.alpha.coreos.com/public-ip: 互联IP</li>
</ul>

<p>有了上述信息，flannel就可以在不同的node之间建立overlay网络，采用的就是vxlan技术。</p>

<p>查看node1上的fdb表：</p>

<pre><code class="language-shell"># bridge fdb  |grep flannel.1
52:91:cb:aa:d1:bd dev flannel.1 dst 10.212.36.170 self permanent
02:74:40:05:9f:87 dev flannel.1 dst 10.212.36.169 self permanent
</code></pre>

<p>上述是node1上的fdb表，分别是转发到node2上和node3上。最后的permanent表示该fdb永远不会超时。从这里可以看出，该fdb是通过flannel来维护的，当集群中有新的node加入时，其上的flannel会申请一个新的subnet，该信息会通知到所有的flannel节点上，flannel会在fdb表中追加新的内容。</p>

<p>删除一个节点的话，同理。也是会从该fdb表中修改对应的条目。</p>

<p>查看node1上flannel.1设备的neigh：</p>

<pre><code class="language-shell"># ip neigh show dev flannel.1
172.168.1.0 lladdr 02:74:40:05:9f:87 PERMANENT
172.168.2.0 lladdr 52:91:cb:aa:d1:bd PERMANENT
</code></pre>

<p>其中172.168.1.0是node2上flannel.1的地址。</p>

<p>172.168.2.0是node3上flannel.1的地址。</p>

<h4 id="cni0">cni0</h4>

<p>查看cni0的一些信息：</p>

<pre><code class="language-shell"># brctl show cni0
bridge name     bridge id               STP enabled     interfaces
cni0            8000.0a58aca80001       no              veth0c81e625
                                                        veth3752bd50
                                                        vethf12090da
</code></pre>

<p>可见cni0是一个linux bridge设备，上面挂在了三个容器的网卡对端设备。</p>

<p>查看其ip neigh：</p>

<pre><code class="language-shell"># ip neigh show dev cni0
172.168.0.5 lladdr 0a:58:ac:a8:00:05 STALE
172.168.0.4 lladdr 0a:58:ac:a8:00:04 REACHABLE
172.168.0.6 lladdr 0a:58:ac:a8:00:06 STALE
</code></pre>

<p>上述三个IP就是调度到该节点上的三个pod的IP。</p>

<h4 id="route信息">route信息</h4>

<p>最后来看一下宿主机上的路由信息</p>

<pre><code class="language-shell"># ip r
default via 10.212.36.254 dev eth0 ## 默认路由
10.212.36.0/24 dev eth0  proto kernel  scope link  src 10.212.36.168 ## 宿主机网卡的路由
169.254.0.0/16 dev eth0  scope link  metric 1002 
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 
172.168.0.0/24 dev cni0  proto kernel  scope link  src 172.168.0.1 ## （1）
172.168.1.0/24 via 172.168.1.0 dev flannel.1 onlink ## （2） 
172.168.2.0/24 via 172.168.2.0 dev flannel.1 onlink ## （3）
</code></pre>

<ul>
<li>（1）：172.168.0.0/24是node1申请过来的subnet，理论上调度到该节点的pod都会从该C段地址内分配IP。因为所有的pod都是桥接到cni0上的，因此该条路由通过cni0转发给pod。</li>
<li>（2）：172.168.1.0/24是node2申请过来的subnet，那么如果目的地址是该网段内的流量，则通过flannel.1设备发送给172.168.1.0，172.168.1.0是node2上的flannel.1的地址，flannel.1网卡将数据包vxlan封装之后发送给node2上的flannel.1。node1上的flannel.1是如何找到node2上的flannel.1呢，通过fdb表，上面已经说明。</li>
<li>（3）同（2）。</li>
</ul>

<h3 id="容器内网络情况">容器内网络情况</h3>

<p>下面进入容器查看一下：</p>

<pre><code class="language-shell"># kubectl exec -it test-75b789cbdc-vdsg4 ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default 
    link/ether 0a:58:ac:a8:00:06 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.168.0.6/24 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre>

<p>容器内的eth0网卡具有IP地址172.168.0.6，在宿主机申请的IP段（172.168.0.0/24）内。</p>

<p>查看容器内的路由信息：</p>

<pre><code class="language-shell"># kubectl exec -it test-75b789cbdc-vdsg4 ip r
default via 172.168.0.1 dev eth0 
172.168.0.0/24 dev eth0 proto kernel scope link src 172.168.0.6 
172.168.0.0/16 via 172.168.0.1 dev eth0 
</code></pre>

<p>默认网关是172.168.0.1，这个地址是在cni0上的，将网桥作为自己的网关。</p>

<p>如果都是172.168.0.0/24段内的通信，则直接走172.168.0.6。</p>

<p>整个pod段（172.168.0.0/16）内的通信，走网关（cni0）地址172.168.0.1 。</p>

<h3 id="总结">总结</h3>

<p>通过上述分析，集群内pod通信就很清晰了：</p>

<ol>
<li>如果是同一个节点上的pod通信，直接通过linux br转发即可；</li>
<li>如果是跨节点pod通信，需要通过flannl.1 vxlan设备，封包之后发送给对端宿主机上的flannel.1设备。</li>
</ol>
<ul class="pa0">
  
</ul>
<div class="mt6">
      
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "qsyqian" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
      <div id="commento"></div>
<script defer src="https://cdn.commento.io/js/commento.js"></script>

      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://qsyqian.github.io/" >
    &copy;  qianshuangyang 2019 
  </a>
    <div>











</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
